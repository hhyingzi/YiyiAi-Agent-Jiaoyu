{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 安装所有依赖\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple python-dotenv ipykernel llama-index nest_asyncio\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple timm\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple llama-index-llms-huggingface\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple llama-index-embeddings-huggingface\n",
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple llama-index-embeddings-instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OpenAI Key 方案 #####\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "    \n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# test chat\n",
    "response = llm.complete(\"香蕉的颜色是\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 本地模型加载方案 #####\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# 加载本地的qwen2-7b，你本地的模型放在哪就直接改路径\n",
    "llm = HuggingFaceLLM(\n",
    "    tokenizer_name=\"G:\\\\models\\\\Qwen2-7B-Instruct\",\n",
    "    model_name=\"G:\\\\models\\\\Qwen2-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"trust_remote_code\": True},\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "# 调用本地bce-embedding-base_v1作为embedding模型\n",
    "embed_args = {\n",
    "    'model_name': 'hkunlp/instructor-base', \n",
    "    'max_length': 512, \n",
    "    'embed_batch_size': 32, \n",
    "    'device': 'cuda'\n",
    "    }\n",
    "embed_model = HuggingFaceEmbedding(**embed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 加载PDF数据，这里雄哥以卖油翁为例，可以改为自己的任意数据放在datasets文件夹即可，或改为指定的绝对路径\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/maiyouweng.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 把文本分块，chunk_size为1024，每一块的大小\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# 创建文档的节点\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里可以打印每一个块的信息！\n",
    "node_metadata = nodes[0].get_content(metadata_mode=True)\n",
    "print(len(nodes), node_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# 创建数据摘要索引\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# 创建矢量存储索引\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把这些索引，转换为工具，后面雄哥就可以调用这些工具\n",
    "# 摘要查询引擎\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# 向量查询引擎\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# 定义summary_tool的工具\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"适用于生成与卖油翁课堂相关的摘要问题。\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 定义vector_tool的工具\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"适用于检索卖油翁常规上下文的问题。\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "# 定义所有的tools\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"卖油翁课堂有什么内容?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"卖油翁的文章引发了什么思考？\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.source_nodes))\n",
    "for node in response.source_nodes:\n",
    "    print(node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
