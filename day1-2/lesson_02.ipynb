{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OpenAI Key 方案 #####\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "    \n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# test chat\n",
    "response = llm.complete(\"香蕉的颜色是\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 本地模型加载方案 #####\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# 加载本地的qwen2-7b，你本地的模型放在哪就直接改路径\n",
    "llm = HuggingFaceLLM(\n",
    "    tokenizer_name=\"G:\\\\models\\\\Qwen2-7B-Instruct\",\n",
    "    model_name=\"G:\\\\models\\\\Qwen2-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"trust_remote_code\": True},\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "# 调用本地bce-embedding-base_v1作为embedding模型\n",
    "embed_args = {\n",
    "    'model_name': 'hkunlp/instructor-base', \n",
    "    'max_length': 512, \n",
    "    'embed_batch_size': 32, \n",
    "    'device': 'cuda'\n",
    "    }\n",
    "embed_model = HuggingFaceEmbedding(**embed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"将两个数相加\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# substraction function\n",
    "def sub(x: int, y: int) -> int:\n",
    "    \"\"\"两个数字相减\"\"\"\n",
    "    return x - y\n",
    "\n",
    "# multiplication function\n",
    "def mul(x: int, y: int) -> int:\n",
    "    \"\"\"两个数字相乘\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# get user information\n",
    "def get_user_info(name: str) -> str:\n",
    "    \"\"\"Get user information.\"\"\"\n",
    "    data = {\n",
    "        \"雄哥\": {\n",
    "            \"age\": 18,\n",
    "            \"location\": \"广东\"\n",
    "        },\n",
    "        \"小胖\": {\n",
    "            \"age\": 60,\n",
    "            \"location\": \"广东\"\n",
    "        }\n",
    "    }\n",
    "    return f'名字 {name}, 年龄 {data[name][\"age\"]} 来自 {data[name][\"location\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "addition_tool = FunctionTool.from_defaults(fn=add)\n",
    "get_user_info_tool = FunctionTool.from_defaults(fn=get_user_info)\n",
    "multiplication_tool = FunctionTool.from_defaults(fn=mul)\n",
    "substraction_tool = FunctionTool.from_defaults(fn=sub)\n",
    "\n",
    "tools = [addition_tool, get_user_info_tool, multiplication_tool, substraction_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里试试上面定义的函数，大模型能否正常使用\n",
    "response = llm.predict_and_call(\n",
    "    tools, \n",
    "    \"5乘以5等于多少？\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再试一下，确定能正常使用\n",
    "response = llm.predict_and_call(\n",
    "    tools, \n",
    "    \"雄哥多少岁？\", \n",
    "    verbose=True  # 把思考的过程也进行打印输出\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 加载PDF数据，这里雄哥以卖油翁为例，可以改为自己的任意数据放在datasets文件夹即可，或改为指定的绝对路径\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/maiyouweng.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 把文本分块，chunk_size为1024，每一块的大小\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# 创建文档的节点\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 这里可以打印每一个块的信息！\n",
    "node_metadata = nodes[0].get_content(metadata_mode=True)\n",
    "print(node_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# 创建数据摘要索引\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# 创建矢量存储索引\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "# Create vector search query engine\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"康肃公陈尧咨善于射箭，曾经，他在家里场地射箭，然后发生了什么事？\", \n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.source_nodes))\n",
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_search_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"使用以下参数在索引中进行向量搜索：\n",
    "\n",
    "    query (str): 这是你想要在索引中嵌入和搜索的文本字符串\n",
    "    page_numbers (List[str]): 这个参数允许你将搜索限制到特定的页面。如果留空，搜索将包含索引中的所有页面。如果指定了页码，搜索将只包括那些页面\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_search_tool\",\n",
    "    fn=vector_search_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"第二页提到了什么内容？\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确认系统在哪里检索的数据，返回数据源\n",
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"关于卖油翁的摘要生成\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    \"第一页提到了什么内容？\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text()[:10])\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"给我这篇卖油翁课堂的摘要！\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text()[:10])\n",
    "    print(\"=============Text=============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
